{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/media/navid/e068187d-d49f-4d11-8083-d6b72b15843d/data_process_coordinate_extract/100_Class_Data_Studio/SRITY/STUDIO/berano'\n",
    "EXCEL_NAME = 'occlusion_test.xlsx'\n",
    "VIDEO_SAVE_DIR = 'jana_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_video_paths(path):\n",
    "    video_paths = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                video_paths.append(os.path.join(root, file))\n",
    "    return video_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data, path):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = []\n",
    "# frame_counts = []\n",
    "# frame_numbers = []\n",
    "# occlusion_comments = []\n",
    "# left_hand_coordinates_count = []\n",
    "# right_hand_coordinates_count = []\n",
    "\n",
    "videos = get_all_video_paths(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(VIDEO_SAVE_DIR):\n",
    "    os.makedirs(VIDEO_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for video_path in tqdm(videos[:5]):\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     while cap.isOpened():\n",
    "#         success, image = cap.read()\n",
    "#         if not success:\n",
    "#             break\n",
    "#         image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         results = hands.process(image_rgb)\n",
    "        \n",
    "#         if results.multi_hand_landmarks:\n",
    "#             for hand_landmarks in results.multi_hand_landmarks:\n",
    "#                 # Check occlusion of fingers\n",
    "#                 thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "#                 index_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "#                 middle_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "#                 ring_finger_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
    "#                 pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]       \n",
    "\n",
    "#                 # Compare the z-coordinates of finger tips\n",
    "#                 if thumb_tip.z < index_finger_tip.z and thumb_tip.z < middle_finger_tip.z and thumb_tip.z < ring_finger_tip.z and thumb_tip.z < pinky_tip.z:\n",
    "#                     occlusion_comments.append('Thumb is occluded')\n",
    "#                     file_paths.append(video_path)\n",
    "#                     frame_numbers.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "#                     frame_counts.append(frame_count)\n",
    "\n",
    "#                 if index_finger_tip.z < thumb_tip.z and index_finger_tip.z < middle_finger_tip.z and index_finger_tip.z < ring_finger_tip.z and index_finger_tip.z < pinky_tip.z:\n",
    "#                     occlusion_comments.append('Index finger is occluded')\n",
    "#                     file_paths.append(video_path)\n",
    "#                     frame_numbers.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "#                     frame_counts.append(frame_count)\n",
    "\n",
    "#                 if middle_finger_tip.z < thumb_tip.z and middle_finger_tip.z < index_finger_tip.z and middle_finger_tip.z < ring_finger_tip.z and middle_finger_tip.z < pinky_tip.z:\n",
    "#                     occlusion_comments.append('Middle finger is occluded')\n",
    "#                     file_paths.append(video_path)\n",
    "#                     frame_numbers.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "#                     frame_counts.append(frame_count)\n",
    "\n",
    "#                 if ring_finger_tip.z < thumb_tip.z and ring_finger_tip.z < index_finger_tip.z and ring_finger_tip.z < middle_finger_tip.z and ring_finger_tip.z < pinky_tip.z:\n",
    "#                     occlusion_comments.append('Ring finger is occluded')\n",
    "#                     file_paths.append(video_path)\n",
    "#                     frame_numbers.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "#                     frame_counts.append(frame_count)\n",
    "\n",
    "#                 if pinky_tip.z < thumb_tip.z and pinky_tip.z < index_finger_tip.z and pinky_tip.z < middle_finger_tip.z and pinky_tip.z < ring_finger_tip.z:\n",
    "#                     occlusion_comments.append('Pinky is occluded')\n",
    "#                     file_paths.append(video_path)\n",
    "#                     frame_numbers.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "#                     frame_counts.append(frame_count)\n",
    "#                 else:\n",
    "#                     occlusion_comments.append('No occlusion')\n",
    "#                     file_paths.append(video_path)\n",
    "#                     frame_numbers.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "#                     frame_counts.append(frame_count)\n",
    "#         else:\n",
    "#             occlusion_comments.append('No hands detected')\n",
    "#             file_paths.append(video_path)\n",
    "#             frame_numbers.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "#             frame_counts.append(frame_count)\n",
    "#         break\n",
    "\n",
    "# # df = pd.DataFrame({\n",
    "# #     'File Path': file_paths,\n",
    "# #     'Frame Number': frame_numbers,\n",
    "# #     'Occlusion Comments': occlusion_comments,\n",
    "# # })\n",
    "# # write_to_excel(df, EXCEL_NAME)\n",
    "\n",
    "# print(len(file_paths))\n",
    "# print(len(frame_numbers))\n",
    "# print(len(occlusion_comments))\n",
    "# print(len(frame_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [05:44<00:00, 20.27s/it]\n"
     ]
    }
   ],
   "source": [
    "frame_numbers = []\n",
    "hand_landmarks_count = []\n",
    "file_paths = []\n",
    "\n",
    "import random\n",
    "\n",
    "# videos = random.sample(videos, 200)\n",
    "\n",
    "for video_path in tqdm(videos):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    file_name = video_path.split('/')[-1]\n",
    "    out_path = os.path.join(VIDEO_SAVE_DIR, file_name)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output = cv2.VideoWriter(out_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "        landmarks_list = []\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(\n",
    "                        color=(0, 255, 255), thickness=1, circle_radius=1\n",
    "                    ),\n",
    "                    mp_drawing.DrawingSpec(\n",
    "                        color=(255, 255, 0), thickness=1, circle_radius=1\n",
    "                    ),\n",
    "                )\n",
    "                landmarks_list.append(hand_landmarks.landmark)\n",
    "\n",
    "        # if len(landmarks_list) == 2:\n",
    "        #     hand_landmarks_count.append(len(landmarks_list[0]))\n",
    "        #     frame_numbers.append(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        #     file_paths.append(video_path)\n",
    "\n",
    "\n",
    "        cv2.putText(image, \"Hands Count: \" + str(len(landmarks_list)), (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        # time.sleep(.5)\n",
    "\n",
    "        output.write(image)\n",
    "        output.write(image)\n",
    "        output.write(image)\n",
    "        output.write(image)\n",
    "        output.write(image)\n",
    "        output.write(image)\n",
    "        output.write(image)\n",
    "        # # Display processed frame\n",
    "        cv2.imshow(\"Frame\", image)\n",
    "        if cv2.waitKey(1) == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "output.release()\n",
    "cv2.destroyAllWindows()\n",
    "# data = {\n",
    "#     'File Path': file_paths,\n",
    "#     'Frame Number': frame_numbers,\n",
    "#     'Hand Landmarks Count': hand_landmarks_count,\n",
    "# }\n",
    "\n",
    "# write_to_csv(data, EXCEL_NAME)\n",
    "\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
